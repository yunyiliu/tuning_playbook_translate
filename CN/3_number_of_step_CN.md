Determining the number of steps for each training run

- There are two types of workloads: those that are compute-bound and those that are not.
- When training is compute-bound, training is limited by how long we are willing to wait and not by how much training data we have or some other factor.
	- In this case, if we can somehow train longer or more efficiently, we should see a lower training loss and, with proper tuning, an improved validation loss.
	- In other words, speeding up training is equivalent to improving training and the "optimal" training time is always "as long as we can afford."
	- That said, just because a workload is compute-limited doesn't mean training longer/faster is the only way to improve results.
- When training is not compute-bound, we can afford to train as long as we would like to, and, at some point, training longer doesn't help much (or even causes problematic overfitting).
	- In this case, we should expect to be able to train to very low training loss, to the point where training longer might slightly reduce the training loss, but will not meaningfully reduce the validation loss.
	- Particularly when training is not compute-bound, a more generous training time budget can make tuning easier, especially when tuning learning rate decay schedules, since they have a particularly strong interaction with the training budget.
		- In other words, very stingy training time budgets might require a learning rate decay schedule tuned to perfection in order to achieve a good error rate.
- Regardless of whether a given workload is compute-bound or not, methods that increase the variance of the gradients (across batches) will usually result in slower training progress, and thus may increase the number of training steps required to reach a particular validation loss. High gradient variance can be caused by:
	- Using a smaller batch size
	- Adding data augmentation
	- Adding some types of regularization (e.g. dropout)

ç¡®å®šæ¯ä¸ªè®­ç»ƒè¿è¡Œçš„æ­¥æ•°

- æœ‰ä¸¤ç§ç±»å‹çš„å·¥ä½œè´Ÿè½½ï¼šè®¡ç®—å¯†é›†å‹å’Œéè®¡ç®—å¯†é›†å‹ã€‚
- å½“è®­ç»ƒæ˜¯è®¡ç®—å¯†é›†å‹æ—¶ï¼Œè®­ç»ƒå—åˆ°æˆ‘ä»¬æ„¿æ„ç­‰å¾…çš„æ—¶é—´é™åˆ¶ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ•°æ®é‡æˆ–å…¶ä»–å› ç´ çš„é™åˆ¶ã€‚
	- åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿä»¥æŸç§æ–¹å¼æ›´é•¿æˆ–æ›´æœ‰æ•ˆåœ°è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°æ›´ä½çš„è®­ç»ƒæŸå¤±å’Œé€šè¿‡é€‚å½“çš„è°ƒæ•´ï¼Œæ”¹å–„éªŒè¯æŸå¤±ã€‚
	- æ¢å¥è¯è¯´ï¼ŒåŠ å¿«è®­ç»ƒå°±ç›¸å½“äºæ”¹å–„è®­ç»ƒï¼Œè€Œâ€œæœ€ä½³â€è®­ç»ƒæ—¶é—´å§‹ç»ˆæ˜¯â€œå°½å¯èƒ½é•¿â€ã€‚
	- å°½ç®¡å¦‚æ­¤ï¼Œä»…å› ä¸ºå·¥ä½œè´Ÿè½½å—è®¡ç®—é™åˆ¶å¹¶ä¸æ„å‘³ç€å»¶é•¿/åŠ å¿«è®­ç»ƒæ˜¯æ”¹å–„ç»“æœçš„å”¯ä¸€æ–¹æ³•ã€‚
- å½“è®­ç»ƒä¸æ˜¯è®¡ç®—å¯†é›†å‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è´Ÿæ‹…å¾—èµ·è®­ç»ƒçš„æ—¶é—´ï¼Œä½†åœ¨æŸäº›æ—¶å€™ï¼Œè®­ç»ƒæ—¶é—´è¿‡é•¿å¹¶ä¸èƒ½å¸®åŠ©å¤ªå¤šï¼ˆç”šè‡³ä¼šå¯¼è‡´é—®é¢˜æ€§è¿‡æ‹Ÿåˆï¼‰ã€‚
	- åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥æœŸæœ›èƒ½å¤Ÿå°†è®­ç»ƒæŸå¤±é™åˆ°éå¸¸ä½çš„æ°´å¹³ï¼Œå³ä½¿å»¶é•¿è®­ç»ƒæ—¶é—´å¯èƒ½ä¼šç•¥å¾®é™ä½è®­ç»ƒæŸå¤±ï¼Œä½†ä¸ä¼šæ˜¾è‘—é™ä½éªŒè¯æŸå¤±ã€‚
	- ç‰¹åˆ«æ˜¯å½“è®­ç»ƒä¸å—è®¡ç®—é™åˆ¶æ—¶ï¼Œæ›´å……è£•çš„è®­ç»ƒæ—¶é—´é¢„ç®—å¯ä»¥ä½¿è°ƒæ•´æ›´å®¹æ˜“ï¼Œç‰¹åˆ«æ˜¯åœ¨è°ƒæ•´å­¦ä¹ ç‡è¡°å‡è®¡åˆ’æ—¶ï¼Œå› ä¸ºå®ƒä»¬ä¸è®­ç»ƒé¢„ç®—æœ‰ç€ç‰¹åˆ«å¼ºçš„ç›¸äº’ä½œç”¨ã€‚
		- æ¢å¥è¯è¯´ï¼Œéå¸¸åå•¬çš„è®­ç»ƒæ—¶é—´é¢„ç®—å¯èƒ½éœ€è¦å®Œç¾è°ƒæ•´çš„å­¦ä¹ ç‡è¡°å‡è®¡åˆ’æ‰èƒ½å®ç°è‰¯å¥½çš„é”™è¯¯ç‡ã€‚
- æ— è®ºç»™å®šçš„å·¥ä½œè´Ÿè½½æ˜¯è®¡ç®—é™åˆ¶è¿˜æ˜¯ä¸å—é™åˆ¶ï¼Œå¢åŠ æ¢¯åº¦çš„æ–¹å·®ï¼ˆè·¨æ‰¹æ¬¡ï¼‰çš„æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´è®­ç»ƒè¿›åº¦è¾ƒæ…¢ï¼Œå› æ­¤å¯èƒ½éœ€è¦å¢åŠ è®­ç»ƒæ­¥éª¤æ‰èƒ½è¾¾åˆ°ç‰¹å®šçš„éªŒè¯æŸå¤±ã€‚é«˜æ¢¯åº¦æ–¹å·®å¯èƒ½ç”±ä»¥ä¸‹åŸå› å¼•èµ·ï¼š
	- ä½¿ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°
	- æ·»åŠ æ•°æ®å¢å¼º
	- æ·»åŠ æŸäº›ç±»å‹çš„æ­£åˆ™åŒ–ï¼ˆä¾‹å¦‚ä¸¢å¼ƒï¼‰

Deciding how long to train when training is not compute-bound
- Our main goal is to ensure we are training long enough for the model to reach the best possible result, while avoiding being overly wasteful in the number of training steps.
- When in doubt, err on the side of training longer. Performance should never degrade when training longer, assuming retrospective (optimal) checkpoint selection is used properly and checkpoints are frequent enough.
- Never tune the max_train_steps number in a study. Pick a value and use it for all trials. From these trials, plot the training step that retrospective checkpoint selection finds in order to refine the choice of max_train_steps.
	- For example, if the best step is always during the first 10% of training, then the maximum number of steps is way too high.
	- Alternatively, if the best step is consistently in the last 25% of training we might benefit from training longer and re-tuning the decay schedule.
- The ideal number of training steps can change when the architecture or data changes (e.g. adding data augmentation).
- Below we describe how to pick an initial candidate value for max_train_steps based on the number of steps necessary to "perfectly fit" the training set using a constant learning rate.
	- Note, we are not using the phrase "perfectly fit the training set" in a precise or mathematically well-defined way. It is merely meant as an informal descriptor to indicate a very low training loss.
		- For example, when training with the log loss, absent regularization terms, we might see the training loss keep slowly improving until we reach floating point limits as the network weights grow without bound and the predictions of the model on the training set become increasingly confident. In this case, we might say the model "perfectly fit" the training set around the time the misclassification error reached zero on the training set.
	- The starting value for max_train_steps we find may need to be increased if the amount of gradient noise in the training procedure increases.
		- For example, if data augmentation or regularizers like dropout are introduced to the model.
	- It may be possible to decrease max_train_steps if the training process improves somehow.
		- For example, with a better tuned optimizer or a better tuned learning rate schedule.

å½“è®­ç»ƒä¸å—è®¡ç®—é™åˆ¶æ—¶ï¼Œå¦‚ä½•å†³å®šè®­ç»ƒæ—¶é—´ï¼š

- æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ç¡®ä¿è®­ç»ƒè¶³å¤Ÿé•¿ï¼Œä½¿æ¨¡å‹è¾¾åˆ°æœ€ä½³ç»“æœï¼ŒåŒæ—¶é¿å…åœ¨è®­ç»ƒæ­¥éª¤æ•°é‡ä¸Šè¿‡äºæµªè´¹ã€‚
- å¦‚æœæœ‰ç–‘è™‘ï¼Œåº”è¯¥é€‰æ‹©æ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚å‡è®¾å›é¡¾ï¼ˆæœ€ä¼˜ï¼‰æ£€æŸ¥ç‚¹é€‰æ‹©è¢«æ­£ç¡®ä½¿ç”¨å¹¶ä¸”æ£€æŸ¥ç‚¹è¶³å¤Ÿé¢‘ç¹ï¼Œè®­ç»ƒæ—¶é—´è¶Šé•¿ï¼Œæ€§èƒ½ä¸åº”è¯¥ä¸‹é™ã€‚
- åœ¨ç ”ç©¶ä¸­æ°¸è¿œä¸è¦è°ƒæ•´ max_train_steps æ•°å€¼ã€‚é€‰æ‹©ä¸€ä¸ªæ•°å€¼å¹¶åœ¨æ‰€æœ‰è¯•éªŒä¸­ä½¿ç”¨å®ƒã€‚ä»è¿™äº›è¯•éªŒä¸­ï¼Œå¯ä»¥ç»˜åˆ¶å›é¡¾æ£€æŸ¥ç‚¹é€‰æ‹©æ‰¾åˆ°çš„è®­ç»ƒæ­¥éª¤ï¼Œä»¥ä¾¿ä¼˜åŒ– max_train_steps çš„é€‰æ‹©ã€‚
	- ä¾‹å¦‚ï¼Œå¦‚æœæœ€ä½³æ­¥éª¤å§‹ç»ˆåœ¨è®­ç»ƒçš„å‰10%ä¸­ï¼Œé‚£ä¹ˆæœ€å¤§æ­¥æ•°å¤ªé«˜äº†ã€‚
	- æˆ–è€…ï¼Œå¦‚æœæœ€ä½³æ­¥éª¤ä¸€ç›´åœ¨è®­ç»ƒçš„æœ€å25%ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå—ç›Šäºæ›´é•¿çš„è®­ç»ƒæ—¶é—´å¹¶é‡æ–°è°ƒæ•´è¡°å‡è¿›åº¦ã€‚
- å½“ä½“ç³»ç»“æ„æˆ–æ•°æ®å‘ç”Ÿå˜åŒ–æ—¶ï¼ˆä¾‹å¦‚æ·»åŠ æ•°æ®å¢å¼ºï¼‰ï¼Œç†æƒ³çš„è®­ç»ƒæ­¥æ•°å¯èƒ½ä¼šæ”¹å˜ã€‚
	- ä¸‹é¢æˆ‘ä»¬å°†æè¿°å¦‚ä½•æ ¹æ®ä½¿ç”¨æ’å®šå­¦ä¹ ç‡â€œå®Œç¾æ‹Ÿåˆâ€è®­ç»ƒé›†æ‰€éœ€çš„æ­¥éª¤æ•°é€‰æ‹© max_train_steps çš„åˆå§‹å€™é€‰å€¼ã€‚
		- è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä»¥ç²¾ç¡®æˆ–æ•°å­¦ä¸Šå®šä¹‰è‰¯å¥½çš„æ–¹å¼ä½¿ç”¨â€œå®Œç¾æ‹Ÿåˆè®­ç»ƒé›†â€è¿™ä¸ªçŸ­è¯­ã€‚å®ƒä»…ä»…ä½œä¸ºä¸€ä¸ªéæ­£å¼çš„æè¿°ç¬¦ï¼Œç”¨äºæŒ‡ç¤ºè®­ç»ƒæŸå¤±éå¸¸ä½çš„æƒ…å†µã€‚
			- ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨å¯¹æ•°æŸå¤±è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¦‚æœæ²¡æœ‰æ­£åˆ™åŒ–é¡¹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°è®­ç»ƒæŸå¤±ä¿æŒç¼“æ…¢æ”¹å–„ï¼Œç›´åˆ°ç½‘ç»œæƒé‡æ— é™å¢é•¿ä¸”æ¨¡å‹å¯¹è®­ç»ƒé›†çš„é¢„æµ‹å˜å¾—è¶Šæ¥è¶Šè‡ªä¿¡ï¼Œæ­¤æ—¶æˆ‘ä»¬å¯èƒ½ä¼šè®¤ä¸ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šâ€œå®Œç¾æ‹Ÿåˆâ€ã€‚
		- æ‰¾åˆ°çš„ max_train_steps çš„èµ·å§‹å€¼å¯èƒ½éœ€è¦å¢åŠ ï¼Œå¦‚æœè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦å™ªå£°å¢åŠ ã€‚
			- ä¾‹å¦‚ï¼Œå¦‚æœåœ¨æ¨¡å‹ä¸­å¼•å…¥äº†æ•°æ®å¢å¼ºæˆ–dropoutç­‰æ­£åˆ™åŒ–å™¨ã€‚
		- å¦‚æœè®­ç»ƒè¿‡ç¨‹æœ‰æ‰€æ”¹å–„ï¼Œå¯èƒ½å¯ä»¥é™ä½ max_train_stepsã€‚
			- ä¾‹å¦‚ï¼Œé€šè¿‡æ›´å¥½åœ°è°ƒæ•´ä¼˜åŒ–å™¨æˆ–å­¦ä¹ ç‡è¿›åº¦ã€‚



Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep
[Click to expand]

- This procedure assumes it is possible to not only "perfectly" fit the training set, but to do so using a constant learning rate schedule.
- If it is possible to perfectly fit the entire training set, then there must exist a configuration (with some value of max_train_steps) that perfectly fits the training set; find any such configuration and use its value of max_train_steps as a starting point N.
- Run a constant learning rate sweep (i.e. grid search the learning rate) without data augmentation and without regularization where each trial trains for N steps.
- The number of steps required for the fastest trial in the sweep to reach perfect training performance is our initial guess for max_train_steps.
- NOTE: Bad search spaces can make it possible to engage in self-deception.
	- For example, if all the learning rates in a study are too small, we might incorrectly conclude that a very large value of max_train_steps is necessary.
	- At a minimum, we should check that the optimal learning rate in the study is not at the boundary of the search space.

é€šè¿‡å­¦ä¹ ç‡æ‰«æé€‰æ‹©æœ€å¤§è®­ç»ƒæ­¥éª¤çš„åˆå§‹å€™é€‰ç®—æ³•
[ç‚¹å‡»å±•å¼€]

- è¯¥è¿‡ç¨‹å‡è®¾ä¸ä»…å¯ä»¥â€œå®Œç¾â€æ‹Ÿåˆè®­ç»ƒé›†ï¼Œè€Œä¸”å¯ä»¥ä½¿ç”¨æ’å®šçš„å­¦ä¹ ç‡è®¡åˆ’æ¥å®ç°ã€‚
- å¦‚æœèƒ½å¤Ÿå®Œå…¨æ‹Ÿåˆæ•´ä¸ªè®­ç»ƒé›†ï¼Œé‚£ä¹ˆå¿…é¡»å­˜åœ¨ä¸€ä¸ªé…ç½®ï¼ˆå…·æœ‰æŸä¸ªmax_train_stepså€¼ï¼‰ï¼Œå¯ä»¥å®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒé›†ï¼›æ‰¾åˆ°ä»»ä½•è¿™æ ·çš„é…ç½®å¹¶ä½¿ç”¨å…¶max_train_stepså€¼ä½œä¸ºèµ·ç‚¹Nã€‚
- è¿è¡Œä¸€ä¸ªå¸¸æ•°å­¦ä¹ ç‡æ‰«æï¼ˆå³ç½‘æ ¼æœç´¢å­¦ä¹ ç‡ï¼‰ï¼Œä¸ä½¿ç”¨æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–ï¼Œåœ¨æ¯ä¸ªè¯•éªŒä¸­è®­ç»ƒNæ­¥ã€‚
- åœ¨æ‰«æä¸­è¾¾åˆ°å®Œç¾çš„è®­ç»ƒè¡¨ç°æ‰€éœ€çš„æ­¥éª¤æ•°ï¼Œæ˜¯æˆ‘ä»¬å¯¹max_train_stepsçš„åˆå§‹çŒœæµ‹ã€‚
- æ³¨æ„ï¼šç³Ÿç³•çš„æœç´¢ç©ºé—´å¯èƒ½ä¼šè®©æˆ‘ä»¬é™·å…¥è‡ªæˆ‘æ¬ºéª—ã€‚
	- ä¾‹å¦‚ï¼Œå¦‚æœç ”ç©¶ä¸­çš„æ‰€æœ‰å­¦ä¹ ç‡éƒ½å¤ªå°ï¼Œåˆ™å¯èƒ½ä¼šé”™è¯¯åœ°å¾—å‡ºéœ€è¦éå¸¸å¤§çš„max_train_stepså€¼çš„ç»“è®ºã€‚
	- è‡³å°‘ï¼Œæˆ‘ä»¬åº”è¯¥æ£€æŸ¥ç ”ç©¶ä¸­çš„æœ€ä½³å­¦ä¹ ç‡æ˜¯å¦åœ¨æœç´¢ç©ºé—´çš„è¾¹ç•Œä¸Šã€‚

Deciding how long to train when training is compute-bound
- In some cases, training loss keeps improving indefinitely and our patience and computational resources become the limiting factors.
- If training loss (or even validation loss) keeps improving indefinitely, should we always train as long as we can afford? Not necessarily.
	- We might be able to tune more effectively by running a larger number of shorter experiments and reserving the longest "production length" runs for the models we hope to launch.
	- As the training time for trials approaches our patience limit, tuning experiments become more relevant for our potential launch candidates, but we can complete fewer of them.
	- There are probably many questions we can answer while only training for ~10% of the production length, but there is always a risk that our conclusions at this time limit will not apply to experiments at 20% of the production length, let alone 100%.
- Tuning in multiple rounds with increasing, per-trial training step limits is a sensible approach.
	- We can do as many rounds as we want, but usually 1-3 are the most practical.
	- Essentially, try to obtain as much understanding of the problem as possible using trials with a very quick turnaround time, trading off tuning thoroughness with relevance to the final, longest runs.
	- Once a given per-trial time limit has generated useful insights, we can increase the training time and continue tuning, double-checking our conclusions from the shorter runs as needed.
- As a starting point, we recommend two rounds of tuning:
	- Round 1: Shorter runs to find good model and optimizer hyperparameters.
	- Round 2: Very few long runs on good hyperparameter points to get the final model.
- The biggest question going from Round i â†’ Round i+1 is how to adjust learning rate decay schedules.
	- One common pitfall when adjusting learning rate schedules between rounds is using all the extra training steps with too small of a learning rate.


- åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè®­ç»ƒæŸå¤±ä¼šæ— é™æœŸåœ°æ”¹å–„ï¼Œè€Œæˆ‘ä»¬çš„è€å¿ƒå’Œè®¡ç®—èµ„æºæˆä¸ºé™åˆ¶å› ç´ ã€‚
- å¦‚æœè®­ç»ƒæŸå¤±ï¼ˆç”šè‡³éªŒè¯æŸå¤±ï¼‰æ— é™æœŸåœ°æ”¹å–„ï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥å°½å¯èƒ½åœ°è®­ç»ƒï¼Ÿä¸ä¸€å®šã€‚
	- é€šè¿‡è¿è¡Œæ›´å¤šçš„è¾ƒçŸ­å®éªŒå¹¶å°†æœ€é•¿çš„â€œç”Ÿäº§é•¿åº¦â€è¿è¡Œä¿ç•™ç»™æˆ‘ä»¬å¸Œæœ›å¯åŠ¨çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œè°ƒæ•´ã€‚
	- éšç€è¯•éªŒçš„è®­ç»ƒæ—¶é—´æ¥è¿‘æˆ‘ä»¬çš„è€å¿ƒæé™ï¼Œè°ƒæ•´å®éªŒå¯¹æˆ‘ä»¬æ½œåœ¨çš„å¯åŠ¨å€™é€‰è€…å˜å¾—æ›´åŠ ç›¸å…³ï¼Œä½†æˆ‘ä»¬å¯ä»¥å®Œæˆçš„å®éªŒæ•°é‡è¾ƒå°‘ã€‚
	- åœ¨è®­ç»ƒçº¦10%çš„æ—¶é—´å†…ï¼Œæˆ‘ä»¬å¯èƒ½å¯ä»¥å›ç­”è®¸å¤šé—®é¢˜ï¼Œä½†åœ¨æ­¤æ—¶é—´é™åˆ¶ä¸‹å¾—å‡ºçš„ç»“è®ºå¾€å¾€ä¸é€‚ç”¨äºè®­ç»ƒé•¿åº¦ä¸º20%æˆ–100%çš„å®éªŒã€‚
- ä½¿ç”¨é€æ­¥å¢åŠ çš„æ¯æ¬¡è¯•éªŒè®­ç»ƒæ­¥éª¤é™åˆ¶è¿›è¡Œå¤šè½®è°ƒæ•´æ˜¯æ˜æ™ºçš„æ–¹æ³•ã€‚
	- æˆ‘ä»¬å¯ä»¥è¿›è¡Œä»»æ„å¤šè½®è°ƒæ•´ï¼Œä½†é€šå¸¸æœ€å®ç”¨çš„æ˜¯1-3è½®ã€‚
	- æœ¬è´¨ä¸Šï¼Œå°è¯•ä½¿ç”¨éå¸¸å¿«çš„å‘¨è½¬æ—¶é—´è¿›è¡Œå°è¯•ï¼Œä»¥æ¢å–è°ƒæ•´çš„å½»åº•æ€§å’Œä¸æœ€ç»ˆçš„æœ€é•¿è¿è¡Œçš„ç›¸å…³æ€§ï¼Œå°½å¯èƒ½åœ°äº†è§£é—®é¢˜ã€‚
	- ä¸€æ—¦ç‰¹å®šçš„æ¯æ¬¡è¯•éªŒæ—¶é—´é™åˆ¶ç”Ÿæˆæœ‰ç”¨çš„è§è§£ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¢åŠ è®­ç»ƒæ—¶é—´å¹¶ç»§ç»­è°ƒæ•´ï¼Œå¹¶æ ¹æ®éœ€è¦æ£€æŸ¥è¾ƒçŸ­è¿è¡Œçš„ç»“è®ºã€‚
- ä½œä¸ºèµ·ç‚¹ï¼Œæˆ‘ä»¬å»ºè®®è¿›è¡Œä¸¤è½®è°ƒæ•´ï¼š
	- ç¬¬ä¸€è½®ï¼šè¿è¡Œè¾ƒçŸ­æ—¶é—´ä»¥æ‰¾åˆ°è‰¯å¥½çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨è¶…å‚æ•°ã€‚
	- ç¬¬äºŒè½®ï¼šåœ¨è‰¯å¥½çš„è¶…å‚æ•°ç‚¹ä¸Šè¿›è¡Œå¾ˆå°‘çš„é•¿æ—¶é—´è¿è¡Œä»¥è·å¾—æœ€ç»ˆæ¨¡å‹ã€‚
- ä»ç¬¬iè½®åˆ°ç¬¬i+1è½®çš„æœ€å¤§é—®é¢˜æ˜¯å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡è¡°å‡è®¡åˆ’ã€‚
	- è°ƒæ•´å­¦ä¹ ç‡è®¡åˆ’ä¹‹é—´çš„ä¸€ä¸ªå¸¸è§é™·é˜±æ˜¯ä½¿ç”¨æ‰€æœ‰é¢å¤–çš„è®­ç»ƒæ­¥éª¤å¹¶ä½¿ç”¨è¿‡å°çš„å­¦ä¹ ç‡ã€‚

Round 1
[Click to expand]

- Unfortunately, there is no guarantee that good hyperparameters found in short, incomplete training are still good choices when training length is significantly increased. However, for some kinds of hyperparameters, they are often correlated enough for Round 1 to be useful.
- What hyperparameter values found in shorter runs do we expect to transfer to longer training runs? For all of this, we need more research. But based on what we know so far, here are the authorsâ€™ suspicions in order of decreasing probability of transferring:
	- Very likely to transfer
		- Early training instability can be resolved in the first round of tuning using a smaller number of training steps. Perhaps these hyperparameters are the closest thing to a sure bet for transfer that we have.
			- Warmup length
			- Initialization
	- Likely to transfer
		- Model architecture - A dramatic win in the model architecture will usually transfer, but there are probably many counterexamples.
	- Might transfer
		- Optimization algorithm/optimizer hyperparameters - We think this would "loosely" transfer. Itâ€™s definitely weaker than the things above it.
		- Data augmentation
		- Regularization
			- If it isn't possible to perfectly fit the training set, the model might be in a regime where regularization is unlikely to help very much.
	- Unlikely to transfer
		- Learning rate schedule: unlikely to transfer perfectly.
			- This paper suggests that even decay schedule transfers, but we don't believe this is true in general. Example: Tuning sqrt decay on small # of training steps then extending to large # will result in the majority of training occurring at overly small steps.
				- One can likely do "good enough" with most schedules in the limit of extreme training budget, but noticeable performance improvements can likely be seen if it is tuned.
		- Understanding Short-Horizon Bias in Stochastic Meta-Optimization describes the dangers of trying to pick learning rates myopically.

ç¬¬ä¸€è½®

[ç‚¹å‡»å±•å¼€]

- ä¸å¹¸çš„æ˜¯ï¼Œåœ¨çŸ­æ—¶é—´å†…è¿›è¡Œçš„ä¸å®Œæ•´çš„è®­ç»ƒä¸­æ‰¾åˆ°çš„è‰¯å¥½è¶…å‚æ•°ï¼Œåœ¨è®­ç»ƒæ—¶é•¿æ˜¾è‘—å¢åŠ æ—¶å¹¶ä¸èƒ½ä¿è¯ä»ç„¶æ˜¯è‰¯å¥½çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº›è¶…å‚æ•°ç±»å‹æ¥è¯´ï¼Œå®ƒä»¬é€šå¸¸è¶³å¤Ÿç›¸å…³ï¼Œä»¥è‡³äºç¬¬ä¸€è½®è°ƒæ•´ä»ç„¶æ˜¯æœ‰ç”¨çš„ã€‚
- æˆ‘ä»¬æœŸæœ›å°†å“ªäº›åœ¨çŸ­æ—¶é—´è¿è¡Œä¸­æ‰¾åˆ°çš„è¶…å‚æ•°å€¼è½¬ç§»åˆ°æ›´é•¿çš„è®­ç»ƒè¿è¡Œä¸­ï¼Ÿå…³äºè¿™ä¸€åˆ‡ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ç ”ç©¶ã€‚ä½†æ˜¯æ ¹æ®æˆ‘ä»¬ç›®å‰æ‰€çŸ¥ï¼Œè¿™é‡Œæ˜¯ä½œè€…ä»¬æ€€ç–‘çš„è½¬ç§»æ¦‚ç‡é€’å‡çš„é¡ºåºï¼š
	- å¾ˆå¯èƒ½è½¬ç§»
		- åœ¨ç¬¬ä¸€è½®è°ƒæ•´ä¸­ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ­¥éª¤æ¥è§£å†³æ—©æœŸè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚ä¹Ÿè®¸è¿™äº›è¶…å‚æ•°æ˜¯æˆ‘ä»¬æ‹¥æœ‰çš„æœ€æ¥è¿‘ç¡®å®šçš„è½¬ç§»é€‰æ‹©ã€‚
			- çƒ­èº«é•¿åº¦
			- åˆå§‹åŒ–
	- å¯èƒ½è½¬ç§»
		- æ¨¡å‹ç»“æ„ - æ¨¡å‹ç»“æ„çš„æ˜¾è‘—æ”¹è¿›é€šå¸¸ä¼šè½¬ç§»ï¼Œä½†å¯èƒ½å­˜åœ¨è®¸å¤šåä¾‹ã€‚
	- å¯èƒ½è½¬ç§»
		- ä¼˜åŒ–ç®—æ³•/ä¼˜åŒ–å™¨è¶…å‚æ•° - æˆ‘ä»¬è®¤ä¸ºè¿™å¯èƒ½ä¼šâ€œæ¾æ•£â€è½¬ç§»ã€‚å®ƒè‚¯å®šæ¯”ä¸Šè¿°å†…å®¹è¦å¼±ã€‚
		- æ•°æ®å¢å¼º
		- æ­£åˆ™åŒ–
			- å¦‚æœæ— æ³•å®Œç¾æ‹Ÿåˆè®­ç»ƒé›†ï¼Œæ¨¡å‹å¯èƒ½å¤„äºæ­£åˆ™åŒ–å¾ˆéš¾èµ·ä½œç”¨çš„åŒºåŸŸã€‚
	- ä¸å¤ªå¯èƒ½è½¬ç§»
		- å­¦ä¹ ç‡è°ƒåº¦ï¼šä¸å¤ªå¯èƒ½å®Œç¾è½¬ç§»ã€‚
			- è¿™ç¯‡è®ºæ–‡è¡¨æ˜å³ä½¿è¡°å‡è°ƒåº¦è½¬ç§»ï¼Œä½†æˆ‘ä»¬ä¸ç›¸ä¿¡è¿™åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯æ­£ç¡®çš„ã€‚ä¾‹å¦‚ï¼šåœ¨å°‘é‡çš„è®­ç»ƒæ­¥éª¤ä¸Šè°ƒæ•´sqrtè¡°å‡ï¼Œç„¶åå°†å…¶æ‰©å±•åˆ°å¤§é‡çš„æ­¥éª¤ï¼Œå°†å¯¼è‡´å¤§éƒ¨åˆ†è®­ç»ƒä»¥è¿‡å°çš„æ­¥éª¤è¿›è¡Œã€‚
				- åœ¨æç«¯çš„è®­ç»ƒé¢„ç®—é™åˆ¶ä¸‹ï¼Œå¤§å¤šæ•°è°ƒåº¦å¯èƒ½éƒ½è¶³å¤Ÿå¥½ï¼Œä½†å¦‚æœè¿›è¡Œè°ƒæ•´ï¼Œå¯ä»¥æ˜æ˜¾åœ°çœ‹åˆ°æ€§èƒ½çš„æ”¹å–„ã€‚
		- åœ¨éšæœºå…ƒä¼˜åŒ–ä¸­ç†è§£çŸ­è§†åå·®æè¿°äº†è¯•å›¾çŸ­è§†åœ°é€‰æ‹©å­¦ä¹ ç‡çš„å±é™©ã€‚

Round 2
[Click to expand]

- Run the best hyperparameter configuration from Round 1.
- (Speculation) ğŸ¤– Use the extra steps to extend the period of training at a high learning rate.
	- E.g. if linear schedule then keep the length of the decay fixed from Round 1 and extend the period of constant lr in the 	beginning.
	- For cosine decay, just keep the base lr from Round 1 and extend max_train_steps as in Chinchilla paper.
- More rounds might make sense for teams with very mature modeling and tuning pipelines and very long and expensive production training runs, but they will often be overkill.
	- We've described how to transfer from Step 1 â†’ Step 2. If we didn't care about analysis time and if making efficient use of compute was the overriding concern, then the ideal would be to exponentially increase the length of training runs (and thus the end-to-end time to complete a study) over many different rounds of tuning.
		- At each round we systematically ensure our choices continue to hold up.
		- New ideas go through a pipeline that progressively derisks them using increasingly long-running experiments from Step i to Step i+1.

ç¬¬äºŒè½®
[ç‚¹å‡»å±•å¼€]

- è¿è¡Œç¬¬ä¸€è½®çš„æœ€ä½³è¶…å‚æ•°é…ç½®ã€‚
- ï¼ˆæ¨æµ‹ï¼‰ğŸ¤– ä½¿ç”¨é¢å¤–çš„æ­¥éª¤å»¶é•¿ä»¥é«˜å­¦ä¹ ç‡è®­ç»ƒçš„æ—¶é—´ã€‚
	- ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨çº¿æ€§è®¡åˆ’è¡¨ï¼Œåˆ™ä¿æŒä»ç¬¬ä¸€è½®å¼€å§‹çš„è¡°å‡é•¿åº¦ä¸å˜ï¼Œå¹¶å»¶é•¿å¼€å§‹æ—¶çš„æ’å®šå­¦ä¹ ç‡æœŸé—´ã€‚
	- å¯¹äºä½™å¼¦è¡°å‡ï¼Œåªéœ€ä¿æŒç¬¬ä¸€è½®çš„åŸºç¡€å­¦ä¹ ç‡ï¼Œå¹¶å°†æœ€å¤§è®­ç»ƒæ­¥éª¤å»¶é•¿ï¼Œå¦‚Chinchilla paperä¸­æ‰€è¿°ã€‚
- å¯¹äºå…·æœ‰éå¸¸æˆç†Ÿçš„å»ºæ¨¡å’Œè°ƒä¼˜æµæ°´çº¿ä»¥åŠéå¸¸é•¿å’Œæ˜‚è´µçš„ç”Ÿäº§è®­ç»ƒè¿è¡Œçš„å›¢é˜Ÿæ¥è¯´ï¼Œæ›´å¤šè½®æ¬¡å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä½†å®ƒä»¬ç»å¸¸æ˜¯å¤šä½™çš„ã€‚
	- æˆ‘ä»¬å·²ç»æè¿°äº†å¦‚ä½•ä»æ­¥éª¤1 â†’ æ­¥éª¤2è¿›è¡Œè½¬ç§»ã€‚å¦‚æœæˆ‘ä»¬ä¸å…³å¿ƒåˆ†ææ—¶é—´ï¼Œå¹¶ä¸”åˆ©ç”¨è®¡ç®—æœºçš„æ•ˆç‡æ˜¯ä¸»è¦å…³æ³¨ç‚¹ï¼Œé‚£ä¹ˆç†æƒ³æƒ…å†µæ˜¯åœ¨è®¸å¤šä¸åŒçš„è°ƒä¼˜è½®æ¬¡ä¸­æŒ‡æ•°å¢åŠ è®­ç»ƒæ—¶é—´ï¼ˆå› æ­¤æ˜¯å®Œæˆç ”ç©¶çš„ç«¯åˆ°ç«¯æ—¶é—´ï¼‰ã€‚
		- åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç¡®ä¿æˆ‘ä»¬çš„é€‰æ‹©ä»ç„¶å¯é ã€‚
		- æ–°çš„æƒ³æ³•é€šè¿‡é€æ¸å¢åŠ çš„ä»æ­¥éª¤iåˆ°æ­¥éª¤i+1çš„é•¿æ—¶é—´å®éªŒæµç¨‹è¿›è¡Œé£é™©åˆ†æ•£ã€‚

FAQs
What is the best learning rate decay schedule family?
[Click to expand]

- Itâ€™s an open problem. Itâ€™s not clear how to construct a set of rigorous experiments to confidently answer what the "best" LR decay schedule is.
- Although we don't know the best schedule family, we're confident that itâ€™s important to have some (non-constant) schedule and that tuning it matters.
- Different learning rates work best at different times during the optimization process. Having some sort of schedule makes it more likely for the model to hit a good learning rate.

ä»€ä¹ˆæ˜¯æœ€å¥½çš„å­¦ä¹ ç‡è¡°å‡æ—¶é—´è¡¨æ—ï¼Ÿ

- è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„é—®é¢˜ã€‚ç›®å‰å°šä¸æ¸…æ¥šå¦‚ä½•æ„å»ºä¸€ç»„ä¸¥æ ¼çš„å®éªŒæ¥è‡ªä¿¡åœ°å›ç­”ä»€ä¹ˆæ˜¯â€œæœ€å¥½çš„â€å­¦ä¹ ç‡è¡°å‡æ—¶é—´è¡¨ã€‚
- è™½ç„¶æˆ‘ä»¬ä¸çŸ¥é“æœ€ä½³çš„æ—¶é—´è¡¨æ—ï¼Œä½†æˆ‘ä»¬æœ‰ä¿¡å¿ƒéœ€è¦ä¸€äº›ï¼ˆéå¸¸é‡ï¼‰æ—¶é—´è¡¨ï¼Œå¹¶ä¸”è°ƒæ•´å®ƒå¾ˆé‡è¦ã€‚
- ä¸åŒçš„å­¦ä¹ ç‡åœ¨ä¼˜åŒ–è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µè¡¨ç°æœ€ä½³ã€‚æ‹¥æœ‰æŸç§æ—¶é—´è¡¨ä½¿æ¨¡å‹æ›´æœ‰å¯èƒ½è¾¾åˆ°è‰¯å¥½çš„å­¦ä¹ ç‡ã€‚

Which learning rate decay should I use as a default?
[Click to expand]

Our preference is either linear decay or cosine decay, and a bunch of other schedule families are probably good too.

æˆ‘åº”è¯¥å°†å“ªç§å­¦ä¹ ç‡è¡°å‡ä½œä¸ºé»˜è®¤å€¼ï¼Ÿ

- æˆ‘ä»¬çš„é¦–é€‰æ˜¯çº¿æ€§è¡°å‡æˆ–ä½™å¼¦è¡°å‡ï¼Œå…¶ä»–ä¸€äº›è°ƒåº¦æ—å¯èƒ½ä¹Ÿå¾ˆå¥½

Why do some papers have complicated learning rate schedules?
[Click to expand]

- Itâ€™s not uncommon to see papers with complicated piecewise learning rate (LR) decay schedules.
- Readers often wonder how the authors arrived at such a complicated study.
- Many complicated LR decay schedules are the result of tuning the schedule as a function of the validation set performance in an ad hoc way:
	- Start a single training run with some simple LR decay (or a constant learning rate).
	- Keep training running until the performance seems to stagnate. If this happens, pause training. Resume it with a perhaps steeper LR decay schedule (or smaller constant learning rate) from this point. Repeat this process until the conference/launch deadline.
- Blithely copying the resulting schedule is generally not a good idea since the best particular schedule will be sensitive to a host of other hyperparameter choices.
	- Better to copy the algorithm that produced the schedule, although this is rarely possible when arbitrary human judgment produced the schedule.
- This type of validation-error-sensitive schedule is fine to use if it can be fully automated, but human-in-the-loop schedules that are a function of validation error are brittle and not easily reproducible, so we recommend avoiding them.
	- Before publishing results that used such a schedule, please try to make it fully reproducible.


ä¸ºä»€ä¹ˆä¸€äº›è®ºæ–‡ä¸­æœ‰å¤æ‚çš„å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ

- è®ºæ–‡ä¸­å‡ºç°å¤æ‚çš„åˆ†æ®µå­¦ä¹ ç‡(LR)è¡°å‡è°ƒåº¦å¹¶ä¸ç½•è§ã€‚
- è¯»è€…å¸¸å¸¸ä¼šæƒ³çŸ¥é“ä½œè€…æ˜¯å¦‚ä½•å¾—å‡ºè¿™æ ·å¤æ‚çš„ç ”ç©¶ç»“æœçš„ã€‚
- è®¸å¤šå¤æ‚çš„LRè¡°å‡è°ƒåº¦æ˜¯é€šè¿‡å°†è°ƒåº¦ä½œä¸ºéªŒè¯é›†æ€§èƒ½çš„å‡½æ•°è¿›è¡Œè°ƒæ•´è€Œå¾—åˆ°çš„ï¼š
	- å¼€å§‹ä¸€ä¸ªç®€å•çš„LRè¡°å‡ï¼ˆæˆ–æ’å®šçš„å­¦ä¹ ç‡ï¼‰çš„å•æ¬¡è®­ç»ƒè¿è¡Œã€‚
	- è¿›è¡Œè®­ç»ƒï¼Œç›´åˆ°æ€§èƒ½ä¼¼ä¹åœæ»ã€‚å¦‚æœå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œè¯·æš‚åœè®­ç»ƒã€‚ä»æ­¤æ—¶å¼€å§‹ï¼Œä½¿ç”¨æ›´é™¡å³­çš„LRè¡°å‡è°ƒåº¦ï¼ˆæˆ–æ›´å°çš„æ’å®šå­¦ä¹ ç‡ï¼‰æ¢å¤è®­ç»ƒã€‚é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°ä¼šè®®/å‘å¸ƒæˆªæ­¢æ—¥æœŸã€‚
- ç›²ç›®åœ°å¤åˆ¶ç»“æœè°ƒåº¦é€šå¸¸ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºæœ€ä½³çš„ç‰¹å®šè°ƒåº¦ä¼šå¯¹ä¸€ç³»åˆ—å…¶ä»–è¶…å‚æ•°é€‰æ‹©æ•æ„Ÿã€‚
	- æœ€å¥½å¤åˆ¶ç”Ÿæˆè°ƒåº¦çš„ç®—æ³•ï¼Œä½†å½“ä»»æ„çš„äººç±»åˆ¤æ–­äº§ç”Ÿè°ƒåº¦æ—¶ï¼Œè¿™é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ã€‚
- å¦‚æœå¯ä»¥å®Œå…¨è‡ªåŠ¨åŒ–ä½¿ç”¨è¿™ç§éªŒè¯è¯¯å·®æ•æ„Ÿè°ƒåº¦ï¼Œé‚£ä¹ˆä½¿ç”¨å®ƒæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯é‚£äº›ä½œä¸ºéªŒè¯è¯¯å·®å‡½æ•°çš„äººä¸ºè°ƒåº¦æ˜¯è„†å¼±çš„ï¼Œä¸æ˜“é‡ç°ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®é¿å…ä½¿ç”¨å®ƒä»¬ã€‚
	- åœ¨å‘å¸ƒä½¿ç”¨è¿™ç§è°ƒåº¦çš„ç»“æœä¹‹å‰ï¼Œè¯·å°è¯•ä½¿å…¶å®Œå…¨å¯é‡ç°ã€‚


How should Adamâ€™s hyperparameters be tuned?
[Click to expand]

As discussed above, making general statements about search spaces and how many points one should sample from the search space is very difficult. Note that not all the hyperparameters in Adam are equally important. The following rules of thumb correspond to different "budgets" for the number of trials in a study.
- If < 10 trials in a study, only tune the (base) learning rate.
- If 10-25 trials, tune learning rate and .
If 25+ trials, tune the learning rate, 
 and 
.
If one can run substantially more than 25 trials, additionally tune 
.


å¦‚ä½•è°ƒæ•´Adamçš„è¶…å‚æ•°ï¼Ÿ

- å¦‚ä¸Šæ‰€è¿°ï¼Œå¯¹äºæœç´¢ç©ºé—´ä»¥åŠåº”ä»æœç´¢ç©ºé—´é‡‡æ ·å¤šå°‘ç‚¹çš„ä¸€èˆ¬æ€§é™ˆè¿°éå¸¸å›°éš¾ã€‚è¯·æ³¨æ„ï¼ŒAdamä¸­å¹¶éæ‰€æœ‰è¶…å‚æ•°çš„é‡è¦æ€§ç›¸åŒã€‚ä»¥ä¸‹ç»éªŒæ³•åˆ™å¯¹åº”äºç ”ç©¶ä¸­è¯•éªŒæ•°é‡çš„ä¸åŒâ€œé¢„ç®—â€ï¼š

	- å¦‚æœç ”ç©¶ä¸­çš„è¯•éªŒæ¬¡æ•°å°äº10æ¬¡ï¼Œåˆ™ä»…è°ƒæ•´ï¼ˆåŸºæœ¬ï¼‰å­¦ä¹ ç‡ã€‚

	- å¦‚æœæœ‰10-25æ¬¡è¯•éªŒï¼Œåˆ™è°ƒæ•´å­¦ä¹ ç‡å’ŒÎ²1ã€‚

	- å¦‚æœæœ‰25æ¬¡ä»¥ä¸Šçš„è¯•éªŒï¼Œåˆ™è°ƒæ•´å­¦ä¹ ç‡ã€Î²1å’ŒÎ²2ã€‚

	- å¦‚æœå¯ä»¥è¿è¡Œè¿œè¿œè¶…è¿‡25æ¬¡çš„è¯•éªŒï¼Œåˆ™è¿˜éœ€è°ƒæ•´Îµã€‚



Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?
[Click to expand]
- Quasi-random search (based on low-discrepancy sequences) is our preference over fancier black box optimization tools when used as part of an iterative tuning process intended to maximize insight into the tuning problem (what we refer to as the "exploration phase"). Bayesian optimization and similar tools are more appropriate for the exploitation phase.
- Quasi-random search based on randomly shifted low-discrepancy sequences can be thought of as "jittered, shuffled grid search", since it uniformly, but randomly, explores a given search space and spreads out the search points more than random search.
- The advantages of quasi-random search over more sophisticated black box optimization tools (e.g. Bayesian optimization, evolutionary algorithms) include:
	- Sampling the search space non-adaptively makes it possible to change the tuning objective in post hoc analysis without rerunning experiments.
		- For example, we usually want to find the best trial in terms of validation error achieved at any point in training. But the non-adaptive nature of quasi-random search makes it possible to find the best trial based on final validation error, training error, or some alternative evaluation metric without rerunning any experiments.
	- Quasi-random search behaves in a consistent and statistically reproducible way.
		- It should be possible to reproduce a study from six months ago even if the implementation of the search algorithm changes, as long as it maintains the same uniformity properties. If using sophisticated Bayesian optimization software, the implementation might change in an important way between versions, making it much harder to reproduce an old search. It isnâ€™t always possible to roll back to an old implementation (e.g. if the optimization tool is run as a service).
Its uniform exploration of the search space makes it easier to reason about the results and what they might suggest about the search space.
For example, if the best point in the traversal of quasi-random search is at the boundary of the search space, this is a good (but not foolproof) signal that the search space bounds should be changed. This section goes into more depth. However, an adaptive black box optimization algorithm might have neglected the middle of the search space because of some unlucky early trials even if it happens to contain equally good points, since it is this exact sort of non-uniformity that a good optimization algorithm needs to employ to speed up the search.
Running different numbers of trials in parallel versus sequentially will not produce statistically different results when using quasi-random search (or other non-adaptive search algorithms), unlike with adaptive algorithms.
More sophisticated search algorithms may not always handle infeasible points correctly, especially if they aren't designed with neural network hyperparameter tuning in mind.
Quasi-random search is simple and works especially well when many tuning trials will be running in parallel.
Anecdotally1, it is very hard for an adaptive algorithm to beat a quasi-random search that has 2X its budget, especially when many trials need to be run in parallel (and thus there are very few chances to make use of previous trial results when launching new trials).
Without expertise in Bayesian optimization and other advanced black box optimization methods, we might not achieve the benefits they are, in principle, capable of providing. It is hard to benchmark advanced black box optimization algorithms in realistic deep learning tuning conditions. They are a very active area of current research, and the more sophisticated algorithms come with their own pitfalls for inexperienced users. Experts in these methods are able to get good results, but in high-parallelism conditions the search space and budget tend to matter a lot more.
That said, if our computational resources only allow a small number of trials to run in parallel and we can afford to run many trials in sequence, Bayesian optimization becomes much more attractive despite making our tuning results harder to interpret.
ä¸ºä»€ä¹ˆåœ¨è°ƒå‚çš„æ¢ç´¢é˜¶æ®µä½¿ç”¨å‡†éšæœºæœç´¢è€Œä¸æ˜¯æ›´å¤æ‚çš„é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Ÿ

- åœ¨æˆ‘ä»¬çš„è¿­ä»£è°ƒä¼˜è¿‡ç¨‹ä¸­ï¼Œå‡†éšæœºæœç´¢ï¼ˆåŸºäºä½å¤±çœŸåºåˆ—ï¼‰æ˜¯æˆ‘ä»¬æ¯”å–œæ¬¢çš„é»‘ç®±ä¼˜åŒ–å·¥å…·æ›´é€‚åˆäºæ—¨åœ¨æœ€å¤§åŒ–å¯¹è°ƒä¼˜é—®é¢˜çš„æ´å¯ŸåŠ›çš„æ¢ç´¢é˜¶æ®µã€‚è´å¶æ–¯ä¼˜åŒ–å’Œç±»ä¼¼çš„å·¥å…·æ›´é€‚åˆäºåˆ©ç”¨é˜¶æ®µã€‚

- åŸºäºéšæœºç§»ä½çš„ä½å¤±çœŸåºåˆ—çš„å‡†éšæœºæœç´¢å¯ä»¥è¢«è®¤ä¸ºæ˜¯â€œæŠ–åŠ¨çš„ã€æ´—ç‰Œçš„ç½‘æ ¼æœç´¢â€ï¼Œå› ä¸ºå®ƒå‡åŒ€ä½†éšæœºåœ°æ¢ç´¢ç»™å®šçš„æœç´¢ç©ºé—´ï¼Œå¹¶æ¯”éšæœºæœç´¢æ›´å¹¿æ³›åœ°åˆ†å¸ƒæœç´¢ç‚¹ã€‚


- å‡†éšæœºæœç´¢ç›¸æ¯”äºæ›´å¤æ‚çš„é»‘ç›’ä¼˜åŒ–å·¥å…·ï¼ˆä¾‹å¦‚è´å¶æ–¯ä¼˜åŒ–ã€è¿›åŒ–ç®—æ³•ï¼‰çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š
	- éè‡ªé€‚åº”åœ°å¯¹æœç´¢ç©ºé—´è¿›è¡ŒæŠ½æ ·ï¼Œä½¿å¾—åœ¨äº‹ååˆ†æä¸­å¯ä»¥æ›´æ”¹è°ƒæ•´ç›®æ ‡è€Œæ— éœ€é‡æ–°è¿è¡Œå®éªŒã€‚
		- ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›åœ¨è®­ç»ƒçš„ä»»ä½•æ—¶å€™æ‰¾åˆ°åœ¨éªŒè¯è¯¯å·®æ–¹é¢å–å¾—çš„æœ€ä½³è¯•éªŒã€‚ä½†æ˜¯ï¼Œå‡†éšæœºæœç´¢çš„éè‡ªé€‚åº”æ€§ä½¿å¾—åœ¨ä¸é‡æ–°è¿è¡Œä»»ä½•å®éªŒçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åŸºäºæœ€ç»ˆéªŒè¯è¯¯å·®ã€è®­ç»ƒè¯¯å·®æˆ–æŸäº›å…¶ä»–è¯„ä¼°æŒ‡æ ‡æ‰¾åˆ°æœ€ä½³è¯•éªŒã€‚
	- å‡†éšæœºæœç´¢ä»¥ä¸€è‡´å’Œç»Ÿè®¡å¯é‡å¤çš„æ–¹å¼è¡Œä¸ºã€‚
		-å¦‚æœä¿æŒç›¸åŒçš„å‡åŒ€æ€§å±æ€§ï¼Œåº”è¯¥èƒ½å¤Ÿé‡ç°å…­ä¸ªæœˆå‰çš„ç ”ç©¶ï¼Œå³ä½¿æœç´¢ç®—æ³•çš„å®ç°å‘ç”Ÿäº†å˜åŒ–ã€‚å¦‚æœä½¿ç”¨å¤æ‚çš„è´å¶æ–¯ä¼˜åŒ–è½¯ä»¶ï¼Œå®ç°åœ¨ç‰ˆæœ¬ä¹‹é—´å¯èƒ½ä¼šå‘ç”Ÿé‡å¤§å˜åŒ–ï¼Œè¿™ä½¿å¾—éš¾ä»¥é‡ç°æ—§çš„æœç´¢ã€‚æœ‰æ—¶æ— æ³•å›æ»šåˆ°æ—§çš„å®ç°ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä¼˜åŒ–å·¥å…·ä½œä¸ºæœåŠ¡è¿è¡Œï¼‰ã€‚
	- å…¶å¯¹æœç´¢ç©ºé—´çš„å‡åŒ€æ¢ç´¢ä½¿å¾—æ›´å®¹æ˜“æ¨æ–­ç»“æœåŠå…¶å¯¹æœç´¢ç©ºé—´çš„å»ºè®®ã€‚
		- ä¾‹å¦‚ï¼Œå¦‚æœå‡†éšæœºæœç´¢éå†ä¸­çš„æœ€ä½³ç‚¹ä½äºæœç´¢ç©ºé—´çš„è¾¹ç•Œï¼Œåˆ™è¿™æ˜¯ä¸€ä¸ªå¥½çš„ï¼ˆä½†ä¸æ˜¯ç»å¯¹å‡†ç¡®çš„ï¼‰ä¿¡å·ï¼Œè¡¨æ˜åº”è¯¥æ›´æ”¹æœç´¢ç©ºé—´çš„è¾¹ç•Œã€‚æœ¬èŠ‚å°†æ·±å…¥æ¢è®¨æ­¤é—®é¢˜ã€‚ç„¶è€Œï¼Œä¸€ä¸ªè‡ªé€‚åº”çš„é»‘ç›’ä¼˜åŒ–ç®—æ³•å¯èƒ½ä¼šå› ä¸ºæŸäº›ä¸å¹¸çš„æ—©æœŸè¯•éªŒè€Œå¿½ç•¥äº†æœç´¢ç©ºé—´çš„ä¸­é—´éƒ¨åˆ†ï¼Œå³ä½¿å®ƒåŒ…å«åŒæ ·å¥½çš„ç‚¹ï¼Œå› ä¸ºæ­£æ˜¯è¿™ç§ä¸å‡åŒ€æ€§ä½¿å¾—è‰¯å¥½çš„ä¼˜åŒ–ç®—æ³•éœ€è¦ä½¿ç”¨æ¥åŠ é€Ÿæœç´¢ã€‚

- ä½¿ç”¨å‡†éšæœºæœç´¢ï¼ˆæˆ–å…¶ä»–éè‡ªé€‚åº”æœç´¢ç®—æ³•ï¼‰å¹¶è¡Œè¿è¡Œä¸åŒæ•°é‡çš„è¯•éªŒä¸æŒ‰é¡ºåºè¿è¡Œä¸ä¼šäº§ç”Ÿç»Ÿè®¡ä¸Šçš„ä¸åŒç»“æœï¼Œè¿™ä¸è‡ªé€‚åº”ç®—æ³•ä¸åŒã€‚
- æ›´å¤æ‚çš„æœç´¢ç®—æ³•å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†ä¸å¯è¡Œç‚¹ï¼Œç‰¹åˆ«æ˜¯å¦‚æœå®ƒä»¬æ²¡æœ‰è€ƒè™‘ç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒæ•´è€Œè®¾è®¡ã€‚

- å‡†éšæœºæœç´¢ç®€å•æ˜“è¡Œï¼Œç‰¹åˆ«é€‚ç”¨äºè®¸å¤šè°ƒæ•´è¯•éªŒå°†å¹¶è¡Œè¿è¡Œçš„æƒ…å†µã€‚
	- æ®è¯´ï¼Œåœ¨å¾ˆå¤šè¯•éªŒéœ€è¦å¹¶è¡Œè¿è¡Œï¼ˆå› æ­¤åœ¨å¯åŠ¨æ–°è¯•éªŒæ—¶å¾ˆå°‘æœ‰æœºä¼šåˆ©ç”¨ä»¥å‰çš„è¯•éªŒç»“æœï¼‰çš„æƒ…å†µä¸‹ï¼Œå¾ˆéš¾è®©è‡ªé€‚åº”ç®—æ³•å‡»è´¥å…¶é¢„ç®—2å€çš„å‡†éšæœºæœç´¢ã€‚
	- å¦‚æœæ²¡æœ‰è´å¶æ–¯ä¼˜åŒ–å’Œå…¶ä»–é«˜çº§é»‘ç›’ä¼˜åŒ–æ–¹æ³•çš„ä¸“ä¸šçŸ¥è¯†ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•å®ç°å®ƒä»¬åŸåˆ™ä¸Šèƒ½å¤Ÿæä¾›çš„å¥½å¤„ã€‚åœ¨å®é™…æ·±åº¦å­¦ä¹ è°ƒæ•´æ¡ä»¶ä¸‹ï¼Œå¾ˆéš¾å¯¹é«˜çº§é»‘ç›’ä¼˜åŒ–ç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å®ƒä»¬æ˜¯å½“å‰ç ”ç©¶çš„ä¸€ä¸ªéå¸¸æ´»è·ƒçš„é¢†åŸŸï¼Œè€Œæ›´å¤æ‚çš„ç®—æ³•å¯¹äºæ²¡æœ‰ç»éªŒçš„ç”¨æˆ·æ¥è¯´ä¹Ÿæœ‰å…¶è‡ªèº«çš„ç¼ºé™·ã€‚è¿™äº›æ–¹æ³•çš„ä¸“å®¶èƒ½å¤Ÿè·å¾—è‰¯å¥½çš„ç»“æœï¼Œä½†åœ¨é«˜å¹¶è¡Œæ¡ä»¶ä¸‹ï¼Œæœç´¢ç©ºé—´å’Œé¢„ç®—å¾€å¾€æ›´ä¸ºé‡è¦ã€‚

- è¯è™½å¦‚æ­¤ï¼Œå¦‚æœæˆ‘ä»¬çš„è®¡ç®—èµ„æºåªå…è®¸å¹¶è¡Œè¿è¡Œå°‘é‡è¯•éªŒï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥è´Ÿæ‹…è¿è¡Œè®¸å¤šè¯•éªŒåºåˆ—ï¼Œé‚£ä¹ˆè´å¶æ–¯ä¼˜åŒ–å°†å˜å¾—æ›´åŠ æœ‰å¸å¼•åŠ›ï¼Œå°½ç®¡ä¼šä½¿æˆ‘ä»¬çš„è°ƒæ•´ç»“æœæ›´éš¾ä»¥è§£é‡Šã€‚



Where can I find an implementation of quasi-random search?
[Click to expand]

- Open-Source Vizier has an implementation of quasi-ranom search. Set algorithm="QUASI_RANDOM_SEARCH" in this usage example.
- An alternative implementation exists here.
- Both implementations above generate a Halton sequence for a given search space (intended to implement a shifted, scrambled Halton sequence as recommended in https://arxiv.org/abs/1706.03200).
- If a quasi-random search algorithm based on a low-discrepancy sequence is not available, it is possible to substitute pseudo random uniform search instead, although this is likely to be slightly less efficient.
	- In 1-2 dimensions, grid search is also acceptable, although not in higher dimensions (see Bergstra & Bengio, 2012).

æˆ‘åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°å‡†éšæœºæœç´¢çš„å®ç°ï¼Ÿ

- å¼€æºçš„Vizierä¸­æœ‰ä¸€ä¸ªå‡†éšæœºæœç´¢çš„å®ç°ã€‚åœ¨æ­¤ä½¿ç”¨ç¤ºä¾‹ä¸­è®¾ç½®algorithm="QUASI_RANDOM_SEARCH"ã€‚
- å¦å¤–ä¸€ä¸ªå®ç°åœ¨æ­¤å¤„ã€‚
- ä¸Šè¿°ä¸¤ä¸ªå®ç°éƒ½ä¸ºç»™å®šçš„æœç´¢ç©ºé—´ç”ŸæˆHaltonåºåˆ—ï¼ˆæ—¨åœ¨å®ç°https://arxiv.org/abs/1706.03200ä¸­æ¨èçš„åç§»ã€æ··æ·†çš„Haltonåºåˆ—ï¼‰ã€‚
- å¦‚æœåŸºäºä½å·®å¼‚åºåˆ—çš„å‡†éšæœºæœç´¢ç®—æ³•ä¸å¯ç”¨ï¼Œåˆ™å¯ä»¥æ›¿æ¢ä¸ºä¼ªéšæœºå‡åŒ€æœç´¢ï¼Œå°½ç®¡è¿™å¯èƒ½ç•¥å¾®ä½æ•ˆã€‚
	- åœ¨1-2ç»´ä¸­ï¼Œç½‘æ ¼æœç´¢ä¹Ÿæ˜¯å¯æ¥å—çš„ï¼Œä½†åœ¨é«˜ç»´ä¸­ä¸å¯è¡Œï¼ˆå‚è§Bergstraå’ŒBengioï¼Œ2012ï¼‰ã€‚



How many trials are needed to get good results with quasi-random search?
[Click to expand]

A box plot showing the importance of sampling enough

Figure 3: A ResNet-50 was tuned on ImageNet with 100 trials. Via bootstrapping, different amounts of tuning budget were simulated. Box plots of the best performances for each trial budget are plotted above.

There is no way to answer this question in general, but we can look at specific examples.
As the Figure 3 shows, the number of trials in a study can have a substantial impact on the results.
Notice how large the interquartile ranges are when 6 trials were sampled, versus when 20 trials were sampled.
Even with 20 trials, it is likely that the difference between especially lucky and unlucky studies will be larger than the typical variation between re-trains of this model on different random seeds, with fixed hyperparameters, which for this workload might be around +/- 0.1% on a validation error rate of ~23%.

ä½¿ç”¨å‡†éšæœºæœç´¢éœ€è¦å¤šå°‘æ¬¡è¯•éªŒæ‰èƒ½è·å¾—å¥½ç»“æœï¼Ÿ

- æ— æ³•ä¸€èˆ¬æ€§åœ°å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä½†æˆ‘ä»¬å¯ä»¥çœ‹å…·ä½“çš„ä¾‹å­ã€‚
- å¦‚å›¾3æ‰€ç¤ºï¼Œç ”ç©¶ä¸­çš„è¯•éªŒæ¬¡æ•°å¯ä»¥å¯¹ç»“æœäº§ç”Ÿé‡å¤§å½±å“ã€‚
	- è¯·æ³¨æ„ï¼Œå½“é‡‡æ ·6æ¬¡è¯•éªŒæ—¶ï¼Œå››åˆ†ä½è·æ˜¯å¤šä¹ˆå¤§ï¼Œè€Œå½“é‡‡æ ·20æ¬¡è¯•éªŒæ—¶åˆ™ä¸åŒã€‚
	- å³ä½¿é‡‡æ ·äº†20æ¬¡è¯•éªŒï¼Œç‰¹åˆ«æ˜¯å¹¸è¿æˆ–ä¸å¹¸çš„ç ”ç©¶ä¹‹é—´çš„å·®å¼‚å¯èƒ½ä¼šæ¯”è¯¥å·¥ä½œè´Ÿè½½ä¸Šè¯¥æ¨¡å‹åœ¨ä¸åŒéšæœºç§å­ã€å›ºå®šè¶…å‚æ•°ä¸‹é‡æ–°è®­ç»ƒçš„å…¸å‹å·®å¼‚æ›´å¤§ï¼Œåè€…çš„éªŒè¯è¯¯å·®ç‡å¤§çº¦ä¸º23%å·¦å³ï¼Œè¯¯å·®ç‡å˜åŒ–èŒƒå›´å¯èƒ½åœ¨+/- 0.1%å·¦å³ã€‚



How can optimization failures be debugged and mitigated?
[Click to expand]

- Summary: If the model is experiencing optimization difficulties, itâ€™s important to fix them before trying other things. Diagnosing and correcting training failures is an active area of research.

Changing the strides in a single residual block in a WideResnet results in training instability.

- Figure 4: Changing the strides in a single residual block (2x2 -> 1x1) in a WideResnet results in training instability. This does not degrade performance at low learning rates, but high learning rates no longer train well due to the instability. Applying 1000 steps of learning rate warmup resolves this particular instance of instability, allowing stable training at max learning rate of .1.
å¦‚ä½•è°ƒè¯•å’Œç¼“è§£ä¼˜åŒ–å¤±è´¥é—®é¢˜ï¼Ÿ

- æ€»ç»“ï¼šå¦‚æœæ¨¡å‹é‡åˆ°ä¼˜åŒ–å›°éš¾ï¼Œé‡è¦çš„æ˜¯åœ¨å°è¯•å…¶ä»–æ–¹æ³•ä¹‹å‰è§£å†³è¿™äº›å›°éš¾ã€‚è¯Šæ–­å’Œçº æ­£è®­ç»ƒå¤±è´¥æ˜¯ç ”ç©¶çš„ä¸€ä¸ªæ´»è·ƒé¢†åŸŸã€‚
åœ¨WideResnetä¸­æ›´æ”¹å•ä¸ªæ®‹å·®å—çš„æ­¥å¹…ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

- å›¾4ï¼šåœ¨WideResnetä¸­æ›´æ”¹å•ä¸ªæ®‹å·®å—ï¼ˆ2x2-> 1x1ï¼‰çš„æ­¥å¹…ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚è¿™ä¸ä¼šé™ä½ä½å­¦ä¹ ç‡ä¸‹çš„æ€§èƒ½ï¼Œä½†ç”±äºä¸ç¨³å®šæ€§ï¼Œé«˜å­¦ä¹ ç‡ä¸å†è®­ç»ƒè‰¯å¥½ã€‚åº”ç”¨1000æ­¥å­¦ä¹ ç‡é¢„çƒ­å¯ä»¥è§£å†³è¿™ç§ç‰¹å®šçš„ä¸ç¨³å®šæ€§å®ä¾‹ï¼Œä»è€Œå…è®¸åœ¨æœ€å¤§å­¦ä¹ ç‡ä¸º0.1æ—¶ç¨³å®šè®­ç»ƒã€‚

- Identifying unstable workloads
- Any workload will become unstable if the learning rate is too large. Instability is only an issue when it forces us to use a learning rate thatâ€™s too small.
- There are at least two types of training instability worth distinguishing:
	- Instability at initialization/early in training.
	- Sudden instability in the middle of training.
- We can take a systematic approach to identifying stability issues in our workload.
	- Do a learning rate sweep and find the best learning rate lr*.
	- Plot training loss curves for learning rates just above lr*.
	- If the learning rates > lr* show loss instability (loss goes up not down during periods of training), then it is likely that fixing the instability will result in better training.
- Log the L2 norm of the full loss gradient during training, outlier values can result in spurious instability in the middle of training. This can inform how to pick gradient/update clipping.

NOTE: Some models show very early instability followed by a recovery that results in slow but stable training. Common evaluation schedules can miss these issues by not evaluating frequently enough!
- è¯†åˆ«ä¸ç¨³å®šå·¥ä½œè´Ÿè½½
- å¦‚æœå­¦ä¹ ç‡è¿‡å¤§ï¼Œä»»ä½•å·¥ä½œè´Ÿè½½éƒ½ä¼šå˜å¾—ä¸ç¨³å®šã€‚åªæœ‰å½“ä¸ç¨³å®šæ€§è¿«ä½¿æˆ‘ä»¬ä½¿ç”¨å¤ªå°çš„å­¦ä¹ ç‡æ—¶æ‰ä¼šæˆä¸ºé—®é¢˜ã€‚
- è‡³å°‘æœ‰ä¸¤ç§å€¼å¾—åŒºåˆ†çš„è®­ç»ƒä¸ç¨³å®šç±»å‹ï¼š
	- åˆå§‹åŒ–/è®­ç»ƒæ—©æœŸçš„ä¸ç¨³å®šæ€§ã€‚
	- è®­ç»ƒä¸­æœŸçªç„¶å‡ºç°çš„ä¸ç¨³å®šæ€§ã€‚
- æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ç³»ç»Ÿæ€§æ–¹æ³•æ¥è¯†åˆ«å·¥ä½œè´Ÿè½½ä¸­çš„ç¨³å®šæ€§é—®é¢˜ã€‚
	- è¿›è¡Œå­¦ä¹ ç‡æ‰«æï¼Œæ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡lr*ã€‚
	- ç»˜åˆ¶å­¦ä¹ ç‡ç•¥é«˜äºlr*çš„è®­ç»ƒæŸå¤±æ›²çº¿ã€‚
	- å¦‚æœå­¦ä¹ ç‡> lr*æ˜¾ç¤ºæŸå¤±ä¸ç¨³å®šæ€§ï¼ˆåœ¨è®­ç»ƒæœŸé—´æŸå¤±ä¸Šå‡è€Œä¸æ˜¯ä¸‹é™ï¼‰ï¼Œåˆ™ä¿®å¤ä¸ç¨³å®šæ€§å¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„è®­ç»ƒã€‚
- åœ¨è®­ç»ƒæœŸé—´è®°å½•å®Œæ•´æŸå¤±æ¢¯åº¦çš„L2èŒƒæ•°ï¼Œå¼‚å¸¸å€¼å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒä¸­å‡ºç°è™šå‡çš„ä¸ç¨³å®šæ€§ã€‚è¿™å¯ä»¥æŒ‡å¯¼å¦‚ä½•é€‰æ‹©æ¢¯åº¦/æ›´æ–°å‰ªè£ã€‚
æ³¨æ„ï¼šæœ‰äº›æ¨¡å‹æ˜¾ç¤ºå‡ºæ—©æœŸä¸ç¨³å®šæ€§ï¼Œéšåæ¢å¤æ­£å¸¸ï¼Œå¯¼è‡´è®­ç»ƒç¼“æ…¢ä½†ç¨³å®šã€‚å¸¸è§çš„è¯„ä¼°è®¡åˆ’å¯èƒ½ç”±äºè¯„ä¼°é¢‘ç‡ä¸å¤Ÿè€Œå¿½ç•¥è¿™äº›é—®é¢˜ï¼

To check for this, we can train for an abbreviated run of just ~500 steps using lr = 2 * current best, but evaluate every step.
Figure 5: Illustration of the value of more frequent evaluations at the start of training. Useful if thereâ€™s a suspicion that the model suffers from early training instability.

ä¸ºäº†æ£€æŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨lr = 2 *å½“å‰æœ€ä½³å€¼è¿›è¡Œç¼©çŸ­çš„çº¦500ä¸ªæ­¥éª¤çš„è®­ç»ƒï¼Œä½†æ¯æ­¥éƒ½è¿›è¡Œè¯„ä¼°ã€‚
å›¾5ï¼šæ›´é¢‘ç¹åœ°åœ¨è®­ç»ƒå¼€å§‹æ—¶è¿›è¡Œè¯„ä¼°çš„ä»·å€¼è¯´æ˜ã€‚å¦‚æœæ€€ç–‘æ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå­˜åœ¨ä¸ç¨³å®šæ€§ï¼Œåˆ™è¿™æ˜¯æœ‰ç”¨çš„ã€‚

- Potential fixes for common instability patterns
	- Apply learning rate warmup
		- Best for early training instability.
	- Apply gradient clipping
		- Good for both early and mid training instability, may fix some bad inits that warmup cannot.
	- Try a new optimizer
		- Sometimes Adam can handle instabilities that Momentum canâ€™t. This is an active area of research.
	- We can ensure that weâ€™re using best practices/initializations for our model architecture (examples below).
		- Add residual connections and normalization if the model doesn't contain it already.
	- Normalization should be the last operation before the residual. E.g. x + Norm(f(x)).
	- Norm(x + f(x)) known to cause issues.
	- Try initializing residual branches to 0 (e.g. ReZero init).
	- Lower the learning rate
		- This is a last resort.
- Learning rate warmup

- å¸¸è§ä¸ç¨³å®šæ€§æ¨¡å¼çš„æ½œåœ¨ä¿®å¤æ–¹æ¡ˆ
	- åº”ç”¨å­¦ä¹ ç‡é¢„çƒ­
		- æœ€é€‚åˆæ—©æœŸè®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚
	- åº”ç”¨æ¢¯åº¦è£å‰ª
		- æ—¢é€‚åˆæ—©æœŸè®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œä¹Ÿé€‚åˆä¸­æœŸè®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œå¯èƒ½ä¿®å¤ä¸€äº›é¢„çƒ­æ— æ³•è§£å†³çš„é—®é¢˜ã€‚
	- å°è¯•æ–°çš„ä¼˜åŒ–å™¨
		- æœ‰æ—¶Adamèƒ½å¤Ÿå¤„ç†Momentumæ— æ³•å¤„ç†çš„ä¸ç¨³å®šæ€§ã€‚è¿™æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚
	- æˆ‘ä»¬å¯ä»¥ç¡®ä¿æˆ‘ä»¬åœ¨ä½¿ç”¨æœ€ä½³å®è·µ/åˆå§‹åŒ–æ¥å®ç°æˆ‘ä»¬çš„æ¨¡å‹æ¶æ„ï¼ˆç¤ºä¾‹å¦‚ä¸‹ï¼‰ã€‚
		- å¦‚æœæ¨¡å‹ä¸­æ²¡æœ‰ï¼Œæ·»åŠ æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ã€‚
	- å½’ä¸€åŒ–åº”è¯¥æ˜¯åœ¨æ®‹å·®ä¹‹å‰çš„æœ€åä¸€ä¸ªæ“ä½œã€‚ä¾‹å¦‚x + Norm(f(x))ã€‚
	- Norm(x + f(x))è¢«è®¤ä¸ºä¼šå¯¼è‡´é—®é¢˜ã€‚
	- å°è¯•å°†æ®‹å·®åˆ†æ”¯åˆå§‹åŒ–ä¸º0ï¼ˆä¾‹å¦‚ReZeroåˆå§‹åŒ–ï¼‰ã€‚
	- é™ä½å­¦ä¹ ç‡
		- è¿™æ˜¯æœ€åçš„æ‰‹æ®µã€‚
- å­¦ä¹ ç‡é¢„çƒ­

Figure 6: An example of instability during a warmup period (note the horizontal axis log scale). 40k steps of warmup was needed for successful training in this case.

å›¾6ï¼šçƒ­èº«æœŸé—´ä¸ç¨³å®šæ€§çš„ç¤ºä¾‹ï¼ˆè¯·æ³¨æ„æ°´å¹³è½´å¯¹æ•°åˆ»åº¦ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦è¿›è¡Œ40kæ­¥çš„çƒ­èº«æ‰èƒ½æˆåŠŸè®­ç»ƒã€‚

- When to apply learning rate warmup

Figure 7a: An example of a hyperparameter axis plot for a model exhibiting training instability. The best learning rate is at the edge of what is feasible. An "infeasible" trial is defined as one that either produces NaNs or uncharacteristically high values of the loss.
	- Figure 7a shows a hyperparameter axis plot that indicates a model experiencing optimization instabilities, because the best learning rate is right at the edge of instability.
	- Figure 7b shows how this can be double-checked by examining the training loss of a model trained with a learning rate either 5x or 10x larger than this peak. If that plot shows a sudden rise in the loss after a steady decline (e.g. at step ~10k in the figure above), then the model likely suffers from optimization instability.

- ä½•æ—¶åº”ç”¨å­¦ä¹ ç‡çƒ­èº«
å›¾7aï¼šå±•ç¤ºäº†ä¸€ä¸ªæ¨¡å‹è®­ç»ƒä¸ç¨³å®šæ€§çš„è¶…å‚æ•°è½´å›¾ç¤ºã€‚æœ€ä½³å­¦ä¹ ç‡ä½äºå¯è¡ŒèŒƒå›´çš„è¾¹ç¼˜ã€‚åœ¨è¯¥å›¾ä¸­ï¼Œâ€œä¸å¯è¡Œçš„â€è¯•éªŒå®šä¹‰ä¸ºäº§ç”ŸNaNæˆ–å¼‚å¸¸é«˜æŸå¤±å€¼çš„è¯•éªŒã€‚

- å›¾7aæ˜¾ç¤ºäº†ä¸€ä¸ªè¶…å‚æ•°è½´å›¾ç¤ºï¼ŒæŒ‡ç¤ºäº†ä¸€ä¸ªæ¨¡å‹ç»å†äº†ä¼˜åŒ–ä¸ç¨³å®šæ€§ï¼Œå› ä¸ºæœ€ä½³å­¦ä¹ ç‡æ°å¥½å¤„äºä¸ç¨³å®šæ€§çš„è¾¹ç¼˜ã€‚
- å›¾7bå±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ£€æŸ¥ä½¿ç”¨æ¯”è¯¥å³°å€¼å¤§5å€æˆ–10å€çš„å­¦ä¹ ç‡è®­ç»ƒçš„æ¨¡å‹çš„è®­ç»ƒæŸå¤±æ¥è¿›è¡ŒåŒé‡æ£€æŸ¥ã€‚å¦‚æœè¯¥å›¾æ˜¾ç¤ºåœ¨ç¨³å®šä¸‹é™åçªç„¶å‡ºç°æŸå¤±çš„ä¸Šå‡ï¼ˆä¾‹å¦‚åœ¨ä¸Šé¢çš„å›¾ä¸­çš„æ­¥éª¤~10kå¤„ï¼‰ï¼Œåˆ™è¯¥æ¨¡å‹å¯èƒ½é­å—ä¼˜åŒ–ä¸ç¨³å®šæ€§çš„å›°æ‰°ã€‚

- How to apply learning rate warmup
	- Using the section immediately above, we assume that the practitioner has already identified the learning rate at which the model becomes unstable. This is the unstable_base_learning_rate.
	- Warmup involves prepending a learning rate schedule that ramps up the learning rate from 0 to some stable base_learning_rate, that is at least one order of magnitude larger than unstable_base_learning_rate. The default would be to try a base_learning_rate thatâ€™s 10x unstable_base_learning_rate. Although note that itâ€™d be possible to run this entire procedure again for something like 100x unstable_base_learning_rate. The specific schedule is:
		- Ramp up from 0 to base_learning_rate over warmup_steps.
		- Train at a constant rate for post_warmup_steps.
	- Our goal is to find the shortest number of warmup_steps that allows us to access peak learning rates that are much higher than unstable_base_learning_rate.
	- So for each base_learning_rate, we need to tune warmup_steps and post_warmup_steps. Itâ€™s usually fine to set post_warmup_steps to be 2*warmup_steps.
	- Warmup can be tuned independently of an existing decay schedule. warmup_steps should be swept at a few different orders of magnitude. For example, an example study could try [10, 103, 104, 105]. The largest feasible point shouldn't be more than 10% of max_train_steps.
	- Once a warmup_steps that doesn't blow up training at base_learning_rate has been established, it should be applied to the baseline model. Essentially, we prepend this schedule onto the existing schedule, and use the optimal checkpoint selection discussed above to compare this experiment to the baseline. For example, if we originally had 10,000 max_train_steps and did warmup_steps for 1000 steps, the new training procedure should run for 11,000 steps total.
	- If long warmup_steps are required for stable training (>5% of max_train_steps), max_train_steps may need to be increased to account for this.
	- There isn't really a "typical" value across the full range of workloads. Some models only need 100 steps, while others (particularly transformers) may need 40k+.

- å¦‚ä½•åº”ç”¨å­¦ä¹ ç‡é¢„çƒ­
	- å‡è®¾ä»ä¸Šé¢æ‰€è¿°ï¼Œå®è·µè€…å·²ç»ç¡®å®šäº†æ¨¡å‹å˜å¾—ä¸ç¨³å®šçš„å­¦ä¹ ç‡ã€‚è¿™ä¸ªä¸ç¨³å®šçš„åŸºç¡€å­¦ä¹ ç‡å°±æ˜¯ unstable_base_learning_rateã€‚
	- é¢„çƒ­æ¶‰åŠå‡†å¤‡ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦è¡¨ï¼Œå°†å­¦ä¹ ç‡ä»0é€æ­¥å‡é«˜åˆ°ä¸€äº›ç¨³å®šçš„åŸºç¡€å­¦ä¹ ç‡ï¼Œè‡³å°‘æ¯” unstable_base_learning_rate é«˜ä¸€é˜¶ã€‚é»˜è®¤å€¼ä¸ºå°è¯•10å€äº unstable_base_learning_rate çš„åŸºç¡€å­¦ä¹ ç‡ã€‚ä½†è¯·æ³¨æ„ï¼Œå¯ä»¥é’ˆå¯¹åƒ 100 å€ unstable_base_learning_rate è¿™æ ·çš„å€æ•°å†è¿è¡Œæ•´ä¸ªè¿‡ç¨‹ã€‚å…·ä½“çš„æ—¶é—´è¡¨æ˜¯ï¼š
		- åœ¨ warmup_steps ä¸­å°†å­¦ä¹ ç‡ä»0é€æ¸å¢åŠ åˆ° base_learning_rateã€‚
		- åœ¨ post_warmup_steps ä¸­ä»¥æ’å®šé€Ÿç‡è®­ç»ƒã€‚
	- æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€çŸ­çš„ warmup_stepsï¼Œä»¥ä¾¿èƒ½å¤Ÿè®¿é—®è¿œé«˜äº unstable_base_learning_rate çš„æœ€å¤§å­¦ä¹ ç‡ã€‚
	- å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªåŸºç¡€å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ warmup_steps å’Œ post_warmup_stepsã€‚é€šå¸¸å°† post_warmup_steps è®¾ç½®ä¸º 2 * warmup_steps å³å¯ã€‚
	- é¢„çƒ­å¯ä»¥ç‹¬ç«‹äºç°æœ‰è¡°å‡æ—¶é—´è¡¨è¿›è¡Œè°ƒæ•´ã€‚åº”åœ¨å‡ ä¸ªä¸åŒæ•°é‡çº§ä¸‹æ‰«æ warmup_stepsã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç¤ºä¾‹ç ”ç©¶å¯ä»¥å°è¯• [10ã€103ã€104ã€105]ã€‚æœ€å¤§å¯è¡Œç‚¹ä¸åº”è¶…è¿‡ max_train_steps çš„ 10%ã€‚
	- ä¸€æ—¦ç¡®å®šäº†ä¸€ä¸ª warmup_stepsï¼Œåœ¨ base_learning_rate ä¸Šè¿›è¡Œè®­ç»ƒä¸ä¼šå¯¼è‡´è®­ç»ƒå´©æºƒï¼Œå°±åº”å°†å…¶åº”ç”¨äºåŸºçº¿æ¨¡å‹ã€‚å®è´¨ä¸Šï¼Œæˆ‘ä»¬å°†æ­¤æ—¶é—´è¡¨æ·»åŠ åˆ°ç°æœ‰æ—¶é—´è¡¨ä¹‹å‰ï¼Œå¹¶ä½¿ç”¨ä¸Šé¢è®¨è®ºçš„æœ€ä¼˜æ£€æŸ¥ç‚¹é€‰æ‹©æ¥å°†æ­¤å®éªŒä¸åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ€åˆæœ‰ 10,000 ä¸ª max_train_stepsï¼Œå¹¶è¿›è¡Œäº† 1000 ä¸ªæ­¥éª¤çš„ warmup_stepsï¼Œåˆ™æ–°çš„è®­ç»ƒè¿‡ç¨‹åº”è¯¥æ€»å…±è¿è¡Œ 11,000 ä¸ªæ­¥éª¤ã€‚
	- å¦‚æœéœ€è¦é•¿æ—¶é—´çš„ warmup_steps è¿›è¡Œç¨³å®šè®­ç»ƒï¼ˆ> max_train_steps çš„ 5%ï¼‰ï¼Œåˆ™å¯èƒ½éœ€è¦å¢åŠ  max_train_steps æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
	- åœ¨æ•´ä¸ªå·¥ä½œè´Ÿè½½çš„å…¨èŒƒå›´å†…å¹¶æ²¡æœ‰çœŸæ­£çš„â€œå…¸å‹â€å€¼ã€‚æœ‰äº›æ¨¡å‹åªéœ€è¦ 100 æ­¥ï¼Œè€Œå…¶ä»–æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ transformersï¼‰å¯èƒ½éœ€è¦ 40k+ æ­¥ã€‚


- Gradient clipping
- Gradient clipping is most useful when large or outlier gradient issues occur.
- Clipping can fix either early training instability (large gradient norm early), or mid training instabilities (sudden gradient spikes mid training).
- Sometimes longer warmup periods can correct instabilities that clipping does not: see this section above.
	ğŸ¤– What about clipping during warmup?
- The ideal clip thresholds are just above the "typical" gradient norm.
- Hereâ€™s an example of how gradient clipping could be done:
	- If the norm of the gradient ï½œgï½œis greater than the gradient clipping threshold lambda, then do 
 
 where 
 is the new gradient.
- Log the unclipped gradient norm during training. By default, generate:
	- A plot of gradient norm vs step
	- A histogram of gradient norms aggregated over all steps
- Choose a gradient clipping threshold based on the 90th percentile of gradient norms.
	- The threshold will be workload dependent, but 90% is a good starting point. If it doesn't work, this threshold can be tuned.
	- ğŸ¤– What about some sort of adaptive strategy?
- If we try gradient clipping and the instability issues remain, we can try it harder (i.e. make the threshold smaller).
- Extremely aggressive gradient clipping is in essence a strange way of reducing the learning rate. If we find ourselves using extremely aggressive clipping, we probably should just cut the learning rate instead.
- We would usually consider having >50% of the updates getting clipped somehow as "extremely aggressive".
- If we need to do extremely aggressive gradient clipping to deal with our instability issues, then we might as well reduce the learning rate.

- Gradient clippingï¼ˆæ¢¯åº¦è£å‰ªï¼‰
	- å½“æ¢¯åº¦å‡ºç°å¼‚å¸¸å¤§æˆ–ç¦»ç¾¤å€¼æ—¶ï¼Œæ¢¯åº¦è£å‰ªæœ€æœ‰ç”¨ã€‚
	- è£å‰ªå¯ä»¥è§£å†³æ—©æœŸè®­ç»ƒä¸ç¨³å®šæ€§ï¼ˆæ—©æœŸå¤§æ¢¯åº¦èŒƒæ•°ï¼‰æˆ–ä¸­æœŸè®­ç»ƒä¸ç¨³å®šæ€§ï¼ˆä¸­æœŸçªç„¶æ¢¯åº¦æ³¢åŠ¨ï¼‰ã€‚
	- æœ‰æ—¶è¾ƒé•¿çš„é¢„çƒ­æœŸå¯ä»¥çº æ­£è£å‰ªæ— æ³•çº æ­£çš„ä¸ç¨³å®šæ€§ï¼šè¯·å‚è§ä¸Šé¢çš„è¿™èŠ‚ã€‚
	- ğŸ¤–é¢„çƒ­æœŸé—´çš„è£å‰ªæ€ä¹ˆæ ·ï¼Ÿ
	- ç†æƒ³çš„è£å‰ªé˜ˆå€¼ç•¥é«˜äºâ€œå…¸å‹â€æ¢¯åº¦èŒƒæ•°ã€‚
	- è¿™æ˜¯æ¢¯åº¦è£å‰ªçš„ä¸€ä¸ªä¾‹å­ï¼š
		- å¦‚æœæ¢¯åº¦èŒƒæ•° ï½œgï½œå¤§äºæ¢¯åº¦è£å‰ªé˜ˆå€¼ lambdaï¼Œåˆ™è¿›è¡Œä»¥ä¸‹æ“ä½œg^{\prime}=\lambda \times \frac{g}{|g|}
		å…¶ä¸­g^{\prime}
		æ˜¯æ–°æ¢¯åº¦ã€‚

	- åœ¨è®­ç»ƒæœŸé—´è®°å½•æœªè£å‰ªçš„æ¢¯åº¦èŒƒæ•°ã€‚é»˜è®¤ç”Ÿæˆï¼š
		- æ¢¯åº¦èŒƒæ•° vs æ­¥æ•° çš„ç»˜å›¾
		- æ‰€æœ‰æ­¥éª¤çš„æ¢¯åº¦èŒƒæ•°çš„ç›´æ–¹å›¾
	- æ ¹æ®æ¢¯åº¦èŒƒæ•°çš„ç¬¬90ä¸ªç™¾åˆ†ä½æ•°é€‰æ‹©æ¢¯åº¦è£å‰ªé˜ˆå€¼ã€‚
		- é˜ˆå€¼å°†å–å†³äºå·¥ä½œè´Ÿè½½ï¼Œä½†90ï¼…æ˜¯ä¸€ä¸ªå¥½çš„èµ·ç‚¹ã€‚å¦‚æœä¸èµ·ä½œç”¨ï¼Œå¯ä»¥è°ƒæ•´æ­¤é˜ˆå€¼ã€‚
		- ğŸ¤–ä½¿ç”¨æŸç§è‡ªé€‚åº”ç­–ç•¥æ€ä¹ˆæ ·ï¼Ÿ
	- å¦‚æœå°è¯•æ¢¯åº¦è£å‰ªè€Œä¸ç¨³å®šæ€§é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•æ›´ä¸¥æ ¼çš„è£å‰ªï¼ˆå³å‡å°é˜ˆå€¼ï¼‰ã€‚
	- æåº¦æ¿€è¿›çš„æ¢¯åº¦è£å‰ªæœ¬è´¨ä¸Šæ˜¯ä¸€ç§å‡å°‘å­¦ä¹ ç‡çš„å¥‡æ€ªæ–¹å¼ã€‚å¦‚æœæˆ‘ä»¬å‘ç°è‡ªå·±æ­£åœ¨ä½¿ç”¨æåº¦æ¿€è¿›çš„è£å‰ªï¼Œåˆ™å¯èƒ½åº”è¯¥ç›´æ¥é™ä½å­¦ä¹ ç‡ã€‚
	- æˆ‘ä»¬é€šå¸¸è®¤ä¸ºï¼Œå°†>50ï¼…çš„æ›´æ–°ä»¥æŸç§æ–¹å¼è¿›è¡Œè£å‰ªç§°ä¸ºâ€œæåº¦æ¿€è¿›â€ã€‚
	- å¦‚æœæˆ‘ä»¬éœ€è¦è¿›è¡Œæåº¦æ¿€è¿›çš„æ¢¯åº¦è£å‰ªæ¥å¤„ç†ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è€ƒè™‘é™ä½å­¦ä¹ ç‡ã€‚

Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution.
[Click to expand]

- It is true that the term "hyperparameter" has a precise meaning in Bayesian machine learning and referring to the learning rate and most of the other parameters we tune in deep learning as "hyperparameters" is an abuse of terminology.
- We would prefer to use the term "metaparameter" for learning rates, architectural parameters, and all the other things we tune in deep learning, since it avoids the potential for confusion that comes from misusing the word "hyperparameter" (confusion that is especially likely when discussing Bayesian optimization where the probabilistic response surface models have their own true hyperparameters).
- Unfortunately, although potentially confusing, the term hyperparameter has become extremely common in the deep learning community.
- Therefore, for a document, such as this one, intended for a wide audience that includes many people who are unlikely to be aware of this technicality, we made the choice to contribute to one source of confusion in the field in hopes of avoiding another.
That said, we might make a different choice when publishing a research paper, and we would encourage others to use "metaparameter" instead in most contexts.


ä¸ºä»€ä¹ˆä½ æŠŠå­¦ä¹ ç‡å’Œå…¶ä»–ä¼˜åŒ–å‚æ•°ç§°ä¸ºè¶…å‚æ•°ï¼Ÿå®ƒä»¬ä¸æ˜¯ä»»ä½•å…ˆå‰åˆ†å¸ƒçš„å‚æ•°ã€‚

- ç¡®å®ï¼Œåœ¨è´å¶æ–¯æœºå™¨å­¦ä¹ ä¸­ï¼Œâ€œè¶…å‚æ•°â€è¿™ä¸ªæœ¯è¯­æœ‰ä¸€ä¸ªç²¾ç¡®çš„å«ä¹‰ï¼Œå°†å­¦ä¹ ç‡å’Œå¤§å¤šæ•°å…¶ä»–æˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ ä¸­è°ƒæ•´çš„å‚æ•°ç§°ä¸ºâ€œè¶…å‚æ•°â€æ˜¯æœ¯è¯­çš„æ»¥ç”¨ã€‚
- æˆ‘ä»¬æ›´å€¾å‘äºä½¿ç”¨â€œå…ƒå‚æ•°â€è¿™ä¸ªæœ¯è¯­æ¥æè¿°å­¦ä¹ ç‡ã€æ¶æ„å‚æ•°å’Œæ‰€æœ‰å…¶ä»–åœ¨æ·±åº¦å­¦ä¹ ä¸­è°ƒæ•´çš„å‚æ•°ï¼Œå› ä¸ºå®ƒé¿å…äº†é”™è¯¯ä½¿ç”¨â€œè¶…å‚æ•°â€è¿™ä¸ªè¯å¯èƒ½å¸¦æ¥çš„æ··æ·†ï¼ˆç‰¹åˆ«æ˜¯åœ¨è®¨è®ºè´å¶æ–¯ä¼˜åŒ–æ—¶ï¼Œæ¦‚ç‡å“åº”é¢æ¨¡å‹æœ‰è‡ªå·±çš„çœŸå®è¶…å‚æ•°ï¼‰ã€‚
- ä¸å¹¸çš„æ˜¯ï¼Œå°½ç®¡å¯èƒ½ä¼šå¼•èµ·æ··æ·†ï¼Œä½†â€œè¶…å‚æ•°â€è¿™ä¸ªæœ¯è¯­åœ¨æ·±åº¦å­¦ä¹ ç¤¾åŒºä¸­å·²ç»éå¸¸æ™®éã€‚
- å› æ­¤ï¼Œå¯¹äºåƒè¿™æ ·æ—¨åœ¨é¢å‘åŒ…æ‹¬è®¸å¤šä¸å¤ªå¯èƒ½äº†è§£è¿™ç§æŠ€æœ¯ç»†èŠ‚çš„äººç¾¤çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨é¢†åŸŸä¸­ä¸ºä¸€ä¸ªæ¥æºçš„æ··æ·†åšå‡ºè´¡çŒ®ï¼Œä»¥é¿å…å¦ä¸€ä¸ªæ··æ·†ã€‚
å°½ç®¡å¦‚æ­¤ï¼Œå½“å‘å¸ƒç ”ç©¶è®ºæ–‡æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåšå‡ºä¸åŒçš„é€‰æ‹©ï¼Œå¹¶é¼“åŠ±å…¶ä»–äººåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä½¿ç”¨â€œå…ƒå‚æ•°â€ä»£æ›¿â€œè¶…å‚æ•°â€ã€‚


Why shouldn't the batch size be tuned to directly improve validation set performance?
[Click to expand]

- Changing the batch size without changing any other details of the training pipeline will often affect the validation set performance.
- However, the difference in validation set performance between two batch sizes typically goes away if the training pipeline is optimized independently for each batch size.
- The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.
	- Smaller batch sizes introduce more noise into the training algorithm due to sample variance, and this noise can have a regularizing effect. Thus, larger batch sizes can be more prone to overfitting and may require stronger regularization and/or additional regularization techniques.
- In addition, the number of training steps may need to be adjusted when changing the batch size.
- Once all these effects are taken into account, there is currently no convincing evidence that the batch size affects the maximum achievable validation performance (see Shallue et al. 2018).


ä¸ºä»€ä¹ˆæ‰¹é‡å¤§å°ä¸åº”è¯¥è¢«è°ƒæ•´ä»¥ç›´æ¥æé«˜éªŒè¯é›†æ€§èƒ½ï¼Ÿ

- æ”¹å˜æ‰¹é‡å¤§å°è€Œä¸æ”¹å˜è®­ç»ƒç®¡é“çš„ä»»ä½•å…¶ä»–ç»†èŠ‚é€šå¸¸ä¼šå½±å“éªŒè¯é›†æ€§èƒ½ã€‚
- ç„¶è€Œï¼Œå¦‚æœä¸ºæ¯ä¸ªæ‰¹é‡å¤§å°å•ç‹¬ä¼˜åŒ–è®­ç»ƒç®¡é“ï¼Œåˆ™ä¸¤ä¸ªæ‰¹é‡å¤§å°ä¹‹é—´çš„éªŒè¯é›†æ€§èƒ½å·®å¼‚é€šå¸¸ä¼šæ¶ˆå¤±ã€‚
- ä¸æ‰¹é‡å¤§å°æœ€å¼ºçƒˆäº¤äº’çš„è¶…å‚æ•°ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªæ‰¹é‡å¤§å°å•ç‹¬è°ƒæ•´æœ€é‡è¦çš„æ˜¯ä¼˜åŒ–å™¨è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€åŠ¨é‡ï¼‰å’Œæ­£åˆ™åŒ–è¶…å‚æ•°ã€‚
è¾ƒå°çš„æ‰¹é‡å¤§å°ç”±äºæ ·æœ¬æ–¹å·®å¼•å…¥æ›´å¤šçš„å™ªå£°åˆ°è®­ç»ƒç®—æ³•ä¸­ï¼Œè¿™ç§å™ªå£°å¯ä»¥äº§ç”Ÿæ­£åˆ™åŒ–æ•ˆæœã€‚å› æ­¤ï¼Œè¾ƒå¤§çš„æ‰¹é‡å¤§å°å¯èƒ½æ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯èƒ½éœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–å’Œ/æˆ–é¢å¤–çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚
- æ­¤å¤–ï¼Œå½“æ”¹å˜æ‰¹é‡å¤§å°æ—¶ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚
- ä¸€æ—¦è€ƒè™‘äº†æ‰€æœ‰è¿™äº›å½±å“ï¼Œç›®å‰æ²¡æœ‰ä»¤äººä¿¡æœçš„è¯æ®è¡¨æ˜æ‰¹é‡å¤§å°ä¼šå½±å“æœ€å¤§å¯è¾¾åˆ°çš„éªŒè¯æ€§èƒ½ï¼ˆå‚è§Shallueç­‰äºº2018å¹´çš„ç ”ç©¶ï¼‰ã€‚

æ‰€æœ‰æµè¡Œçš„ä¼˜åŒ–ç®—æ³•çš„æ›´æ–°è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ
Stochastic gradient descent (SGD)

\theta_{t+1}=\theta_{t}-\eta_{t} \nabla l\left(\theta_{t}\right)

Momentum

\begin{array}{c}
v_{0}=0 \\
v_{t+1}=\gamma v_{t}+\nabla l\left(\theta_{t}\right) \\
\theta_{t+1}=\theta_{t}-\eta_{t} v_{t+1}
\end{array}

Nesterov

\begin{array}{c}
v_{0}=0 \\
v_{t+1}=\gamma v_{t}+\nabla l\left(\theta_{t}\right) \\
\theta_{t+1}=\theta_{t}-\eta_{t}\left(\gamma v_{t+1}+\nabla l\left(\theta_{t}\right)\right.
\end{array}

RMSProp

\begin{array}{c}
v_{0}=1, m_{0}=0 \\
v_{t+1}=\rho v_{t}+(1-\rho) \nabla l\left(\theta_{t}\right)^{2} \\
m_{t+1}=\gamma m_{t}+\frac{\eta_{t}}{\sqrt{v_{t+1}+\epsilon}} \nabla l\left(\theta_{t}\right) \\
\theta_{t+1}=\theta_{t}-m_{t+1}
\end{array}

ADAM

\begin{array}{c}
m_{0}=0, v_{0}=0 \\
m_{t+1}=\beta_{1} m_{t}+\left(1-\beta_{1}\right) \nabla l\left(\theta_{t}\right) \\
v_{t+1}=\beta_{2} v_{t}+\left(1-\beta_{2}\right) \nabla l\left(\theta_{t}\right)^{2} \\
b_{t+1}=\frac{\sqrt{1-\beta_{2}^{t+1}}}{1-\beta_{1}^{t+1}} \\
\theta_{t+1}=\theta_{t}-\alpha_{t} \frac{m_{t+1}}{\sqrt{v_{t+1}}+\epsilon} b_{t+1}
\end{array}

NADAM

\begin{array}{c}
m_{0}=0, v_{0}=0 \\
m_{t+1}=\beta_{1} m_{t}+\left(1-\beta_{1}\right) \nabla l\left(\theta_{t}\right) \\
v_{t+1}=\beta_{2} v_{t}+\left(1-\beta_{2}\right) \nabla l\left(\theta_{t}\right)^{2} \\
b_{t+1}=\frac{\sqrt{1-\beta_{2}^{t+1}}}{1-\beta_{1}^{t+1}} \\
\theta_{t+1}=\theta_{t}-\alpha_{t} \frac{\beta_{1} m_{t+1}+\left(1-\beta_{1}\right) \nabla l\left(\theta_{t}\right)}{\sqrt{v_{t+1}}+\epsilon} b_{t+1}
\end{array}

















